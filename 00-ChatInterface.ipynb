{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f794a4c2-79e4-4c2a-9410-efd0021a683b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nCreated on Mon Oct 20 20:26:23 2025\\n\\n@author: Koh Chong Ming\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Created on Mon Oct 20 20:26:23 2025\n",
    "\n",
    "@author: Koh Chong Ming\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9d5f5046-08bc-4565-8821-e45fb0863fb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "import gradio as gr\n",
    "from IPython.display import Markdown, display, update_display\n",
    "from openai import OpenAI\n",
    "import time\n",
    "import json\n",
    "\n",
    "#Find the key file\n",
    "\n",
    "os.chdir(\"C:\\\\PythonStuff\\\\keys\")\n",
    "cwd = os.getcwd() \n",
    "\n",
    "with open(\"nebius_api_key\", \"r\") as file:\n",
    "    nebius_api_key = file.read().strip()\n",
    "\n",
    "os.environ[\"NEBIUS_API_KEY\"] = nebius_api_key\n",
    "\n",
    "# Nebius uses the same OpenAI() class, but with additional details\n",
    "nebius_client = OpenAI(\n",
    "    base_url=\"https://api.studio.nebius.ai/v1/\",\n",
    "    api_key=os.environ.get(\"NEBIUS_API_KEY\"),\n",
    ")\n",
    "\n",
    "llama_8b_model = \"meta-llama/Meta-Llama-3.1-8B-Instruct\"\n",
    "llama_70b_model =\"meta-llama/Llama-3.3-70B-Instruct\"\n",
    "gemma_9b_model = \"google/gemma-2-9b-it-fast\"\n",
    "Qwen2_5_72B_model = \"Qwen/Qwen2.5-72B-Instruct\"\n",
    "DeepSeek_V33024 =\"deepseek-ai/DeepSeek-V3-0324\"\n",
    "openai_20b = \"openai/gpt-oss-20b\"\n",
    "Hermes_4_70B_model =\"NousResearch/Hermes-4-70B\"\n",
    "Qwen_Qwen3_Coder=\"Qwen/Qwen3-Coder-480B-A35B-Instruct\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b4787a1c-5911-4d8a-94f3-d0dbd7eafa4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are a helpful assistant\n"
     ]
    }
   ],
   "source": [
    "system_prompt = \"You are a helpful assistant\"\n",
    "\n",
    "print(system_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cd8f26a9-d635-493d-ba60-d32397c6452b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat(message, history, \n",
    "         client=nebius_client,\n",
    "         max_tokens=2056,\n",
    "         temperature=0.7,\n",
    "         model=Qwen_Qwen3_Coder,\n",
    "         use_stream=False):\n",
    "    \n",
    "    #system_message = \"You are a helpful assistant that answers questions using context if provided.\"\n",
    "\n",
    "    messages = [{\"role\": \"system\", \"content\": system_prompt}] + history + [{\"role\": \"user\", \"content\": message}]\n",
    "\n",
    "    # --------------------------\n",
    "    # ðŸŸ¢ STREAMING MODE\n",
    "    # --------------------------\n",
    "    if use_stream:\n",
    "        stream = client.chat.completions.create(\n",
    "            model=model,\n",
    "            messages=messages,\n",
    "            max_tokens=max_tokens,\n",
    "            temperature=temperature,\n",
    "            stream=True\n",
    "        )\n",
    "\n",
    "        response = \"\"\n",
    "        for chunk in stream:\n",
    "            delta = chunk.choices[0].delta.content or \"\"\n",
    "            response += delta\n",
    "            yield response\n",
    "\n",
    "    # --------------------------\n",
    "    # ðŸ”µ NON-STREAMING MODE\n",
    "    # --------------------------\n",
    "    else:\n",
    "        completion = client.chat.completions.create(\n",
    "            model=model,\n",
    "            messages=messages,\n",
    "            max_tokens=max_tokens,\n",
    "            temperature=temperature,\n",
    "            stream=False\n",
    "        )\n",
    "        response = completion.choices[0].message.content\n",
    "        yield response  # or return response if you prefer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "249bebe0-fd18-43d4-98a4-0136928480e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7863\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7863/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ---------------- Custom Baby Blue UI ----------------\n",
    "\n",
    "custom_css = \"\"\"\n",
    "body {\n",
    "    background-color: #b3daff; /* baby blue */\n",
    "}\n",
    "\n",
    ".gradio-container {\n",
    "    background-color: #b3daff !important;\n",
    "    font-family: 'Segoe UI', Arial, sans-serif;\n",
    "}\n",
    "\n",
    ".message {\n",
    "    border-radius: 16px !important;\n",
    "    padding: 10px 14px !important;\n",
    "    font-size: 16px;\n",
    "    line-height: 1.5;\n",
    "}\n",
    "\n",
    ".user {\n",
    "    background-color: #e6f3ff !important;\n",
    "    color: #003366 !important;\n",
    "}\n",
    "\n",
    ".assistant {\n",
    "    background-color: #f0f8ff !important;\n",
    "    color: #002244 !important;\n",
    "    border: 1px solid #cce0ff;\n",
    "}\n",
    "\n",
    "footer, .footer {\n",
    "    background-color: #b3daff !important;\n",
    "}\n",
    "\n",
    "button, .gr-button {\n",
    "    background-color: #80c1ff !important;\n",
    "    color: white !important;\n",
    "    border-radius: 10px !important;\n",
    "    border: none !important;\n",
    "    transition: 0.3s;\n",
    "}\n",
    "\n",
    "button:hover, .gr-button:hover {\n",
    "    background-color: #66b3ff !important;\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "gr.ChatInterface(\n",
    "    fn=chat,\n",
    "    type=\"messages\",\n",
    "    title=\"ðŸ›« LLM Chat Interface\",\n",
    "    description=\"Ask about code or a simple conversation\",\n",
    "    theme=\"soft\",\n",
    "    css=custom_css,\n",
    ").launch(debug=True, inbrowser=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbdd3789-6ffd-4d48-9e10-5d9acb5da7ef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa1deb25-b179-4ca0-8345-1ae25bcb4e7a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
