{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cf42ec1e-2d0d-4cbb-96fb-c65c6109cedf",
   "metadata": {},
   "source": [
    "# Connecting to Nebius LLM via API\n",
    "Setup connection to Nebius API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "79655531-cddc-4642-bea5-45b4e78185c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import requests\n",
    "from IPython.display import Markdown, display\n",
    "from bs4 import BeautifulSoup\n",
    "from openai import OpenAI\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "import time\n",
    "\n",
    "#Find the source file and port to dataframe\n",
    "\n",
    "os.chdir(\"C:\\\\Users\\\\vital\\\\PythonStuff\\\\keys\")\n",
    "cwd = os.getcwd() \n",
    "\n",
    "with open(\"nebius_api_key\", \"r\") as file:\n",
    "    nebius_api_key = file.read().strip()\n",
    "\n",
    "os.environ[\"NEBIUS_API_KEY\"] = nebius_api_key\n",
    "\n",
    "# Nebius uses the same OpenAI() class, but with additional details\n",
    "nebius_client = OpenAI(\n",
    "    base_url=\"https://api.studio.nebius.ai/v1/\",\n",
    "    api_key=os.environ.get(\"NEBIUS_API_KEY\"),\n",
    ")\n",
    "\n",
    "llama_8b_model = \"meta-llama/Meta-Llama-3.1-8B-Instruct\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c381989e-4384-46a8-bd8b-7988c019f833",
   "metadata": {},
   "source": [
    "# A Class to represent a Webpage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "822e0a5a-7c69-4b9e-b0cb-f0b30c7af0eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Website:\n",
    "    def __init__(self, url):\n",
    "        self.url = url\n",
    "        self.title = \"\"\n",
    "        self.text = \"\"\n",
    "        self.scrape()\n",
    "\n",
    "    def scrape(self):\n",
    "        try:\n",
    "            # Chrome options\n",
    "            chrome_options = Options()\n",
    "            chrome_options.add_argument(\"--headless\")\n",
    "            chrome_options.add_argument(\"--no-sandbox\")\n",
    "            chrome_options.add_argument(\"--disable-dev-shm-usage\")\n",
    "            chrome_options.add_argument(\"--disable-gpu\")\n",
    "            chrome_options.add_argument(\"--window-size=1920,1080\")\n",
    "            chrome_options.add_argument(\"--user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36\")\n",
    "\n",
    "            # Try to find Chrome\n",
    "            chrome_paths = [\n",
    "                r\"C:\\Program Files\\Google\\Chrome\\Application\\chrome.exe\",\n",
    "                r\"C:\\Program Files (x86)\\Google\\Chrome\\Application\\chrome.exe\",\n",
    "                r\"C:\\Users\\{}\\AppData\\Local\\Google\\Chrome\\Application\\chrome.exe\".format(os.getenv('USERNAME')),\n",
    "            ]\n",
    "\n",
    "            chrome_binary = None\n",
    "            for path in chrome_paths:\n",
    "                if os.path.exists(path):\n",
    "                    chrome_binary = path\n",
    "                    break\n",
    "\n",
    "            if chrome_binary:\n",
    "                chrome_options.binary_location = chrome_binary\n",
    "\n",
    "            # Create driver\n",
    "            driver = webdriver.Chrome(options=chrome_options)\n",
    "            driver.set_page_load_timeout(30)\n",
    "\n",
    "            print(f\"🔍 Loading: {self.url}\")\n",
    "            driver.get(self.url)\n",
    "\n",
    "            # Wait for page to load\n",
    "            time.sleep(5)\n",
    "\n",
    "            # Try to wait for main content\n",
    "            try:\n",
    "                WebDriverWait(driver, 10).until(\n",
    "                    EC.presence_of_element_located((By.TAG_NAME, \"main\"))\n",
    "                )\n",
    "            except Exception:\n",
    "                try:\n",
    "                    WebDriverWait(driver, 10).until(\n",
    "                        EC.presence_of_element_located((By.TAG_NAME, \"body\"))\n",
    "                    )\n",
    "                except Exception:\n",
    "                    pass  # Continue anyway\n",
    "\n",
    "            # Get title and page source\n",
    "            self.title = driver.title\n",
    "            page_source = driver.page_source\n",
    "            driver.quit()\n",
    "\n",
    "            print(f\"✅ Page loaded: {self.title}\")\n",
    "\n",
    "            # Parse with BeautifulSoup\n",
    "            soup = BeautifulSoup(page_source, 'html.parser')\n",
    "\n",
    "            # Remove unwanted elements\n",
    "            for element in soup([\"script\", \"style\", \"img\", \"input\", \"button\", \"nav\", \"footer\", \"header\"]):\n",
    "                element.decompose()\n",
    "\n",
    "            # Get main content\n",
    "            main = soup.find('main') or soup.find('article') or soup.find('.content') or soup.find('body')\n",
    "            if main:\n",
    "                self.text = main.get_text(separator=\"\\n\", strip=True)\n",
    "            else:\n",
    "                self.text = soup.get_text(separator=\"\\n\", strip=True)\n",
    "\n",
    "            # Clean up text\n",
    "            lines = [line.strip() for line in self.text.split('\\n') if line.strip() and len(line.strip()) > 2]\n",
    "            self.text = '\\n'.join(lines[:200])  # Limit to first 200 lines\n",
    "\n",
    "            print(f\"📄 Extracted {len(self.text)} characters\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error occurred: {e}\")\n",
    "            self.title = \"Error occurred\"\n",
    "            self.text = \"Could not scrape website content\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "25f3684f-70a8-45ad-b3f9-dcfdd7c08704",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ed = Website(\"https://openai.com\")\n",
    "#print(ed.title)\n",
    "#print(ed.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "67ed5590-e4fe-4f31-8592-259076ea1aae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to build the user prompt for LLM\n",
    "def user_prompt_for(website):\n",
    "    user_prompt = f\"You are looking at a website titled {website.title}\"\n",
    "    user_prompt += \"\\nThe contents of this website is as follows; \" \\\n",
    "                   \"please provide a short summary of this website in markdown. \" \\\n",
    "                   \"If it includes news or announcements, then summarize these too.\\n\\n\"\n",
    "    user_prompt += website.text\n",
    "    return user_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "28b8ac1a-38aa-4507-99f6-87763e8a40db",
   "metadata": {},
   "outputs": [],
   "source": [
    "defineSystemPrompt = \"You are an assistant that analyzes the contents of a website \\\n",
    "and provides a short summary, ignoring text that might be navigation related. \\\n",
    "Respond in markdown.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "bf843747-7ecb-428d-b3cf-ca5e56483242",
   "metadata": {},
   "outputs": [],
   "source": [
    "def answer_with_llm(prompt: str,\n",
    "                    system_prompt=defineSystemPrompt,\n",
    "                    max_tokens=512,\n",
    "                    client=nebius_client,\n",
    "                    model=llama_8b_model,\n",
    "                    prettify=True,\n",
    "                    temperature=None) -> str:\n",
    "\n",
    "    messages = []\n",
    "\n",
    "    if system_prompt:\n",
    "        messages.append(\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": system_prompt\n",
    "            }\n",
    "        )\n",
    "\n",
    "    messages.append(\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": prompt\n",
    "        }\n",
    "    )\n",
    "\n",
    "    completion = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        max_tokens=max_tokens,\n",
    "        temperature=temperature\n",
    "    )\n",
    "\n",
    "    #if prettify:\n",
    "    #    return prettify_string(completion.choices[0].message.content)\n",
    "   # else:\n",
    "    return completion.choices[0].message.content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ff4355d-0948-4212-9e20-80be71190f41",
   "metadata": {},
   "source": [
    "# The function that helps to summarize the website by calling the LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d36e561a-19a0-4d87-9b09-8226b39a3eec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize(url):\n",
    "    website = Website(url)\n",
    "    prompt = user_prompt_for(website)\n",
    "    return answer_with_llm(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e228faac-e91d-4c2a-a102-286dd4b4ae65",
   "metadata": {},
   "source": [
    "# Function to display nicely in Juypter.\n",
    "This does not work in normal python scripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9041521e-049a-4394-b549-38161943a643",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_summary(url):\n",
    "    summary = summarize(url)\n",
    "    display(Markdown(summary))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c169569-c645-4a12-9ddb-802b275f85bb",
   "metadata": {},
   "source": [
    "# Invoke action to summarize the website"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2a1638db-9e72-4b92-9e1d-ce1d59ad064f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**Summary of Edward Donner's Website**\n",
       "===============\n",
       "\n",
       "Edward Donner is a co-founder and CTO of Nebula.io, a company that applies AI to help people discover their potential and pursue their reason for being. He is a code enthusiast and enjoys experimenting with Large Language Models (LLMs).\n",
       "\n",
       "**News and Announcements**\n",
       "-------------------------\n",
       "\n",
       "* **Upcoming Events**\n",
       "\t+ AI in Production: Gen AI and Agentic AI on AWS at scale (September 15, 2025)\n",
       "\t+ 2025 AI Executive Briefing (April 21, 2025)\n",
       "* **Courses and Training**\n",
       "\t+ Connecting my courses – become an LLM expert and leader (May 28, 2025)\n",
       "\t+ The Complete Agentic AI Engineering Course (May 18, 2025)\n",
       "\n",
       "**About the Author**\n",
       "---------------------\n",
       "\n",
       "Edward Donner is a former founder and CEO of AI startup untapt, acquired in 2021. He is also a DJ and amateur electronic music producer."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display_summary(\"https://edwarddonner.com\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f6940d48-780c-47e9-abed-3f826d223b36",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**Website Summary**\n",
       "===============\n",
       "\n",
       "Hugging Face is a community-driven platform that aims to democratize good machine learning, one commit at a time. The website provides a hub for developers, researchers, and enthusiasts to share, collaborate, and learn from each other.\n",
       "\n",
       "**Recent Activity**\n",
       "-------------------\n",
       "\n",
       "* A new Space has been updated by mishig about 4 hours ago.\n",
       "* A new activity has been created by yjernite about 21 hours ago.\n",
       "* A paper has been authored by burtenshaw 2 days ago, titled \"A Cartography of Open Collaboration in Open Source AI: Mapping Practices, Motivations, and Governance in 14 Open Large Language Model Projects\".\n",
       "\n",
       "**Community**\n",
       "-------------\n",
       "\n",
       "* The Hugging Face community has 62,207 members and is growing rapidly.\n",
       "* The community is building the future of AI and machine learning together.\n",
       "\n",
       "**Models and Datasets**\n",
       "------------------------\n",
       "\n",
       "* The website provides a collection of pre-trained models, including DistilBERT, which has been updated on May 6, 2024.\n",
       "* There are also 34 datasets available, including documentation images, transformer metadata, and policy documents.\n",
       "\n",
       "**Spaces**\n",
       "------------\n",
       "\n",
       "* Spaces are interactive environments where users can experiment with models and datasets.\n",
       "* There are 32 Spaces available, including the Inference Playground, AI Deadlines, and Number Tokenization Blog.\n",
       "\n",
       "**News and Announcements**\n",
       "---------------------------\n",
       "\n",
       "* Organizations can now publish blog articles on the Hugging Face platform.\n",
       "* The website has experienced significant growth in the past few months, with a 171% increase in team members and a 158% increase in organization card views."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display_summary(\"https://huggingface.co/huggingface\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b4acc7a2-0a30-444a-8937-67d8ba075f22",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**Summary of Just a moment...**\n",
       "\n",
       "This website appears to be a placeholder or a temporary page indicating that the user is waiting for a response from openai.com. The content is minimal, consisting of a single sentence advising the user to enable JavaScript and cookies to continue. There are no news or announcements on this website."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display_summary(\"https://openai.com\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1fb374f-3307-43b4-bd6e-fddc362378f9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
