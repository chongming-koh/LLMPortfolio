{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "cd1b4763-3445-48c9-b675-15cbaa0e9e2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "from IPython.display import Markdown, display, update_display\n",
    "from bs4 import BeautifulSoup\n",
    "from openai import OpenAI\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf42ec1e-2d0d-4cbb-96fb-c65c6109cedf",
   "metadata": {},
   "source": [
    "# Connecting to Nebius LLM via API\n",
    "Setup connection to Nebius API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "79655531-cddc-4642-bea5-45b4e78185c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Find the key file\n",
    "\n",
    "os.chdir(\"C:\\\\Users\\\\vital\\\\PythonStuff\\\\keys\")\n",
    "cwd = os.getcwd() \n",
    "\n",
    "with open(\"nebius_api_key\", \"r\") as file:\n",
    "    nebius_api_key = file.read().strip()\n",
    "\n",
    "os.environ[\"NEBIUS_API_KEY\"] = nebius_api_key\n",
    "\n",
    "# Nebius uses the same OpenAI() class, but with additional details\n",
    "nebius_client = OpenAI(\n",
    "    base_url=\"https://api.studio.nebius.ai/v1/\",\n",
    "    api_key=os.environ.get(\"NEBIUS_API_KEY\"),\n",
    ")\n",
    "\n",
    "llama_8b_model = \"meta-llama/Meta-Llama-3.1-8B-Instruct\"\n",
    "llama_70b_model =\"meta-llama/Llama-3.3-70B-Instruct\"\n",
    "gemma_9b_model = \"google/gemma-2-9b-it-fast\"\n",
    "Qwen2_5_72B_model = \"Qwen/Qwen2.5-72B-Instruct\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "0b0666e9-eec5-4870-835b-2d633fc732b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "userPrompt = \"\"\"\n",
    "Please explain what this code does and why:\n",
    "yield from {book.get(\"author\") for book in books if book.get(\"author\")}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24446f0c-f703-4bba-8de1-e0c1c8d25267",
   "metadata": {},
   "source": [
    "# Function to chat with LLM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "bf843747-7ecb-428d-b3cf-ca5e56483242",
   "metadata": {},
   "outputs": [],
   "source": [
    "def answer_with_llm(prompt: str,\n",
    "                    system_prompt=\"You are an experience tutor with extensive experience in python code, C++, Java and software development lifecycle.\",\n",
    "                    max_tokens=512,\n",
    "                    client=nebius_client,\n",
    "                    model=llama_8b_model,\n",
    "                    prettify=True,\n",
    "                    stream=False,\n",
    "                    displayMode=\"markdown\",\n",
    "                    temperature=None) -> str:\n",
    "\n",
    "    messages = []\n",
    "\n",
    "    if system_prompt:\n",
    "        messages.append(\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": system_prompt\n",
    "            }\n",
    "        )\n",
    "\n",
    "    messages.append(\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": prompt\n",
    "        }\n",
    "    )\n",
    "    if not stream:\n",
    "        completion = client.chat.completions.create(\n",
    "            model=model,\n",
    "            messages=messages,\n",
    "            max_tokens=max_tokens,\n",
    "            temperature=temperature\n",
    "        )\n",
    "        return completion.choices[0].message.content\n",
    "\n",
    "    # ‚úÖ Streaming mode\n",
    "    stream_resp = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        max_tokens=max_tokens,\n",
    "        temperature=temperature,\n",
    "        stream=True\n",
    "    )\n",
    "\n",
    "    \n",
    "    print(f\"üîÅ Streaming from model: {model}\\n\")\n",
    "    full_text = \"\"\n",
    "\n",
    "    # Only create Markdown display handle if needed\n",
    "    md_handle = None\n",
    "    if displayMode == \"markdown\":\n",
    "        md_handle = display(Markdown(\"\"), display_id=True)\n",
    "\n",
    "    # ‚úÖ Single loop\n",
    "    for chunk in stream_resp:\n",
    "        if chunk.choices and chunk.choices[0].delta.content:\n",
    "            token = chunk.choices[0].delta.content\n",
    "            full_text += token\n",
    "\n",
    "            if displayMode == \"markdown\":\n",
    "                update_display(Markdown(full_text), display_id=md_handle.display_id)\n",
    "            elif displayMode == \"console\":\n",
    "                print(token, end=\"\", flush=True)\n",
    "\n",
    "    print(\"\\n‚úÖ Done streaming.\\n\")\n",
    "    return None\n",
    "    \n",
    "    '''\n",
    "    full_text = \"\"\n",
    "    print(f\"üîÅ Streaming from model: {model}\\n\")\n",
    "    for chunk in stream_resp:\n",
    "        if chunk.choices and chunk.choices[0].delta.content:\n",
    "            token = chunk.choices[0].delta.content\n",
    "            print(token, end=\"\", flush=True)\n",
    "            full_text += token\n",
    "\n",
    "    print(\"\\n‚úÖ Done streaming.\\n\")\n",
    "    return full_text\n",
    "    '''\n",
    "    #if prettify:\n",
    "    #    return prettify_string(completion.choices[0].message.content)\n",
    "   # else:\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "dc41e868-1679-4744-b0aa-a21cc3c72ed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_answer(prompt,model,stream,displayMode):\n",
    "    summary = answer_with_llm(prompt, model=model, stream=stream, displayMode=displayMode)\n",
    "    if not stream:\n",
    "        if displayMode == \"markdown\":\n",
    "            display(Markdown(summary))\n",
    "        else:\n",
    "            print(summary)\n",
    "    #display(Markdown(summary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "a7728d12-62e8-47e9-ac1a-84ede5ab8539",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÅ Streaming from model: meta-llama/Meta-Llama-3.1-8B-Instruct\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "This is a Python code snippet that uses a generator expression to yield a filtered list of book authors. Let's break it down:\n",
       "\n",
       "- `yield from`: This is a special syntax in Python that allows a generator to yield values from another iterable. It's often used in conjunction with the `yield` keyword, which is used to create generators. \n",
       "\n",
       "- `{book.get(\"author\") for book in books if book.get(\"author\")}`: This is a set comprehension (or more accurately, a generator expression, since it's wrapped in parentheses and not curly brackets). \n",
       "\n",
       "  - `book.get(\"author\")`: This gets the value associated with the key `\"author\"` from the `book` dictionary. If the key doesn't exist, it returns `None` instead of raising a `KeyError`.\n",
       "\n",
       "  - `for book in books`: This iterates over each book in the `books` collection.\n",
       "\n",
       "  - `if book.get(\"author\")`: This is a conditional statement that filters out any `book` dictionaries that don't have an `\"author\"` key.\n",
       "\n",
       "The entire expression is wrapped in a generator expression, which means it doesn't create a list of all the authors in memory at once. Instead, it lazily generates the authors one at a time as they're requested. \n",
       "\n",
       "Here's why this code might be used:\n",
       "\n",
       "- **Efficiency**: If the `books` collection is very large, it would be inefficient to create a list of all the authors at once. This code allows you to iterate over the authors without having to store them all in memory.\n",
       "\n",
       "- **Lazy Evaluation**: It only evaluates the expression when an author is requested, which can be useful if you're working with large datasets and only need to access a few authors at a time.\n",
       "\n",
       "- **Readability**: The `yield from` syntax makes it clear that this code is yielding values from another iterable, which can make the code easier to read and understand.\n",
       "\n",
       "Here's a simple example of how you might use this code:\n",
       "\n",
       "```python\n",
       "books = [\n",
       "    {\"title\": \"Book 1\", \"author\": \"Author 1\"},\n",
       "    {\"title\": \"Book 2\"},\n",
       "    {\"title\": \"Book 3\", \"author\": \"Author 3\"}\n",
       "]\n",
       "\n",
       "for author in (book.get(\"author\") for book in books if book.get(\"author\")):\n",
       "    print(author)\n",
       "```\n",
       "\n",
       "This will output:\n",
       "\n",
       "```\n",
       "Author 1\n",
       "Author 3\n",
       "```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ Done streaming.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "display_answer(userPrompt,llama_8b_model,True,\"markdown\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "1cd2829b-1173-45f7-92be-a9467590dd16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÅ Streaming from model: Qwen/Qwen2.5-72B-Instruct\n",
      "\n",
      "Certainly! Let's break down the Python code snippet you provided:\n",
      "\n",
      "```python\n",
      "yield from {book.get(\"author\") for book in books if book.get(\"author\")}\n",
      "```\n",
      "\n",
      "### Explanation\n",
      "\n",
      "1. **Set Comprehension**:\n",
      "   - `{book.get(\"author\") for book in books if book.get(\"author\")}` is a set comprehension. It creates a set of unique author names from a list (or other iterable) of `books`.\n",
      "   - `book.get(\"author\")` retrieves the value associated with the key `\"author\"` from each `book` dictionary.\n",
      "   - The `if book.get(\"author\")` condition ensures that only books that have a non-`None` and non-empty author are included in the set. This is because `get` returns `None` if the key is not found, and `None` evaluates to `False` in a boolean context.\n",
      "\n",
      "2. **`yield from`**:\n",
      "   - `yield from` is a syntax in Python used to delegate to another generator or iterable. It allows you to yield all values from an iterable without having to write a loop.\n",
      "   - In this case, it is yielding all the unique author names from the set comprehension.\n",
      "\n",
      "### What Does the Code Do?\n",
      "\n",
      "- **Iterate Over Books**: The code iterates over each `book` in the `books` collection.\n",
      "- **Filter Non-Empty Authors**: For each `book`, it checks if the author is present and not `None` or empty.\n",
      "- **Create a Set of Unique Authors**: It collects these authors into a set, ensuring that each author appears only once.\n",
      "- **Yield Each Author**: Finally, it yields each unique author name from the set.\n",
      "\n",
      "### Example Usage\n",
      "\n",
      "Let's consider an example to see how this works in practice:\n",
      "\n",
      "```python\n",
      "books = [\n",
      "    {\"title\": \"Book A\", \"author\": \"Author 1\"},\n",
      "    {\"title\": \"Book B\", \"author\": \"Author 2\"},\n",
      "    {\"title\": \"Book C\", \"author\": \"Author 1\"},\n",
      "    {\"title\": \"Book D\", \"author\": None},\n",
      "    {\"title\": \"Book E\", \"author\": \"\"}\n",
      "]\n",
      "\n",
      "def get_unique_authors(books):\n",
      "    yield from {book.get(\"author\") for book in books if book.get(\"author\")}\n",
      "\n",
      "# Using the generator function\n",
      "unique_authors = list(get_unique_authors(books))\n",
      "print(unique_authors)\n",
      "```\n",
      "\n",
      "### Output\n",
      "\n",
      "```python\n",
      "['Author 1', '\n",
      "‚úÖ Done streaming.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "display_answer(userPrompt,Qwen2_5_72B_model,True,\"console\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "66289e88-6aa5-45d1-b5cd-92bbe37e33df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "This code snippet demonstrates a powerful Python feature called **generators** and how they can be used in conjunction with **list comprehensions** to efficiently process data. Let's break it down step by step:\n",
       "\n",
       "**Understanding the Components**\n",
       "\n",
       "1. **`yield from`**: This is the heart of the generator. It tells Python that the function should yield one value at a time, pausing its execution until the next value is requested. The `from` keyword means it will yield values from another iterable (in this case, the set comprehension).\n",
       "\n",
       "2. **`{book.get(\"author\") for book in books if book.get(\"author\")}`**:  This is a set comprehension, a concise way to create a set of values. It iterates through a collection named `books`.\n",
       "\n",
       "    *  **`for book in books`**: This loops through each element (`book`) in the `books` collection.\n",
       "    *  **`if book.get(\"author\")`**:  This condition checks if the current `book` has a key called \"author\". The `.get(\"author\")` method safely retrieves the value associated with the \"author\" key. If the key doesn't exist, it returns `None`. The `if` statement ensures that only books with a defined author are included.\n",
       "    * **`book.get(\"author\")`**:  For each book that passes the condition, it extracts and includes the value associated with the \"author\" key in the resulting set.\n",
       "\n",
       "**Putting It Together**\n",
       "\n",
       "This code snippet effectively does the following:\n",
       "\n",
       "   1. **Filters:** It iterates through the `books` collection and keeps only those books that have an \"author\" specified.\n",
       "\n",
       "   2. **Extracts:** For each filtered book, it extracts the author's name.\n",
       "\n",
       "   3. **Yields:** It yields each extracted author's name one by one, creating a generator that produces a sequence of unique authors.\n",
       "\n",
       "**Why Use This?**\n",
       "\n",
       "* **Memory Efficiency:** Generators are memory-friendly because they don't store the entire result set in memory at once. They generate values on demand, which is particularly beneficial when dealing with large datasets.\n",
       "* **Lazy Evaluation:** Values are generated only when requested. This can be advantageous if you're not sure if you'll need all the authors or if you want to process them gradually.\n",
       "\n",
       "**Example Usage**\n",
       "\n",
       "```python\n",
       "books = [\n",
       "    {\"title\":"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display_answer(userPrompt,gemma_9b_model,False,\"markdown\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ca4d1e9-7bdf-4148-856e-7c085ee1b20d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
